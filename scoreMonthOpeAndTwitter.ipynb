{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring & Querying 4 weeks of data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring OPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "import datetime\n",
    "import itertools\n",
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "import boto3\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "timer = None\n",
    "def timerStart():\n",
    "    global timer\n",
    "    timer = datetime.datetime.utcnow()\n",
    "\n",
    "def timerEnd():\n",
    "    global timer\n",
    "    print (datetime.datetime.utcnow() - timer)\n",
    "\n",
    "def scanOpeKeys(startKey, endKey):\n",
    "    bucket = '***'\n",
    "    startKey = startKey\n",
    "    endKey = endKey\n",
    "    bucket = boto3.resource('s3').Bucket(bucket)\n",
    "    for obj in bucket.objects.filter(Marker=startKey):\n",
    "        if obj.key < endKey: \n",
    "            yield obj.key\n",
    "        else:\n",
    "            return\n",
    "\n",
    "def fluff(flat):\n",
    "    deep = dict()\n",
    "    for key in flat:\n",
    "        d = deep\n",
    "        path = key.split('/')\n",
    "        while len(path) > 1:\n",
    "            p = path.pop(0)\n",
    "            d[p] = d.get(p, dict())\n",
    "            d = d[p]\n",
    "        d[path[0]] = flat[key]\n",
    "    return deep\n",
    "\n",
    "def readLocFile(f):\n",
    "    f = open(fileLoc)\n",
    "    flatDat = json.loads(f.read())\n",
    "    f.close()\n",
    "    return json.loads(flatDat)\n",
    "\n",
    "def readS3File(key):\n",
    "    s3obj = boto3.resource('s3').Object(bucket_name='***', key=key)\n",
    "    body = s3obj.get()['Body']\n",
    "    art = body.read().decode('utf-8')\n",
    "    body.close()\n",
    "    return json.loads(art)\n",
    "\n",
    "def bundleArticleData(fileLoc):\n",
    "    #f = open(fileLoc)\n",
    "    #flatDat = json.loads(f.read())\n",
    "    #f.close()\n",
    "    try:\n",
    "        flatDat = readS3File(fileLoc)\n",
    "        art = fluff(flatDat)\n",
    "        attrs = art.get('attrs')\n",
    "        editorialRank = -10\n",
    "        autoRank = -10\n",
    "        for k in attrs:\n",
    "            if 'feedRank' in attrs.get(k):\n",
    "                autoRank = int(attrs.get(k)[9:])\n",
    "            if 'source_rank' in attrs.get(k):\n",
    "                editorialRank = int(attrs.get(k)[12:])\n",
    "        publication = art.get('publication',('a'*38+'Unknown'))[38:]\n",
    "        mediaType = art.get('types',{}).get('0','Unknown')\n",
    "        sentDict = art.get('sentences', {})\n",
    "        sentences = []\n",
    "        articleId = art.get('id')\n",
    "        #for artKeys in art:\n",
    "        if publication == 'Unknown' or (mediaType != 'news' and mediaType != 'blog') or (editorialRank == -10 and autoRank == -10):\n",
    "            return Row(failed=fileLoc, flag='missingLNInfo')\n",
    "        for sentKey in sentDict:\n",
    "            decay = .94 ** int(sentKey)\n",
    "            sent = sentDict.get(sentKey)\n",
    "            #sent = art.get(artKeys)\n",
    "            themesDict = sent.get('themes')\n",
    "            entDict = sent.get('entities')\n",
    "            conceptDict = sent.get('concepts')\n",
    "            sentenceText = sent['yield']\n",
    "            mentions = []\n",
    "            if not themesDict == None:\n",
    "                mentions.extend([themesDict[k]['attributes']['TEXT'] for k in themesDict])\n",
    "            if not entDict == None:\n",
    "                mentions.extend([entDict[k].get('normalform') for k in entDict])\n",
    "            #if not conceptDict == None:\n",
    "            #    mentions.extend(conceptDict[k] for k in conceptDict)\n",
    "            mentions = [Row(theme=m, preMentionScore=decay) for m in mentions]\n",
    "            if mentions:\n",
    "                sentences.append(Row(sentenceId='{}.{:04d}'.format(articleId, int(sentKey)), sentenceText=sentenceText, mentions=mentions))\n",
    "        if sentences:\n",
    "            return Row(articleId=articleId, mediaType=mediaType, title=art.get('title', 'Unknown'), author=art.get('authorId', 'Unknown'),\n",
    "                      publication=publication, editorialRank=editorialRank, autoRank=autoRank, sentences=sentences)\n",
    "        else:\n",
    "            return Row(failed=fileLoc, flag='noSentences')\n",
    "    except:\n",
    "        return Row(failed=fileLoc, flag='parseSomething')\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "def createTable(table, data, schema=None, samplingRatio=None):\n",
    "    '''\n",
    "    Creates a pyspark.sql data frame from 'data'.\n",
    "    Registers it as a temp table named 'table'.\n",
    "    Returns the data frame.\n",
    "    '''\n",
    "    t = sqlContext.createDataFrame(data, schema, samplingRatio)\n",
    "    t.registerTempTable(table)\n",
    "    return t\n",
    "\n",
    "def selectInto(table, sqlQuery):\n",
    "    '''\n",
    "    Runs the 'sqlQuery' in the sqlContext to produce a data frame.\n",
    "    Registers the data frame as a temp table named 'table'. \n",
    "    Returns the data frame.\n",
    "    '''\n",
    "    t = sqlContext.sql(sqlQuery)\n",
    "    t.registerTempTable(table)\n",
    "    return t\n",
    "\n",
    "def showTable(table, n=20, truncate=True, sqlEpilogue=''):\n",
    "    '''Shows a pyspark.sql table -- useful if you don't have the data frame in a variable.'''\n",
    "    sqlContext.sql('select * from {} {}'.format(table, sqlEpilogue)).show(n=n, truncate=truncate)\n",
    "\n",
    "\n",
    "def makeMention(**keys):\n",
    "    return Row(**keys)\n",
    "\n",
    "def makeMentions(themes, decay):\n",
    "    return [makeMention(theme=theme, preMentionScore=decay) for theme in themes]\n",
    "\n",
    "def makeSentence(**keys):\n",
    "    return Row(**keys)\n",
    "\n",
    "def text2sentences(articleId, articleText):\n",
    "    '''Generates mention-bearing sentences from text using Pez.'''\n",
    "    decay = 1.0\n",
    "    for i, sentence in enumerate(pezCast.value.process_document(articleText)):\n",
    "        decay *= .94 \n",
    "        mentions = makeMentions(sentence['entities'] + sentence['concepts'], decay)\n",
    "        if mentions:\n",
    "            yield makeSentence(\n",
    "                sentenceId=u'{}.{:04d}'.format(articleId, i), sentenceText=sentence['text'], mentions=mentions)\n",
    "\n",
    "def dict2article(article):\n",
    "    '''\n",
    "    Generates Article Row from a dictionary representing Moreover json.\n",
    "    Generates nothing if not English, not News/Blog, or no sentences with mentions.\n",
    "    '''\n",
    "    articleId = article['id']\n",
    "    language = article.get('language', 'English')\n",
    "    if language != 'English':\n",
    "        return\n",
    "    title = article.get('title', u'Unknown')\n",
    "    author = article.get('author', {}).get('name', u'Unknown')\n",
    "    articleText = article.get('content', u'')[0:3000]\n",
    "    source = article.get('source', {})\n",
    "    feed = source.get('feed', {})\n",
    "    publication = source.get('name', u'Unknown')\n",
    "    mediaType = feed.get('mediaType', u'Unknown')\n",
    "    if mediaType not in {'News', 'Blog'}:\n",
    "        return\n",
    "    editorialRank = int(source.get('editorialRank', 0))\n",
    "    autoRank = int(feed.get('rank', {}).get('autoRank', 0))\n",
    "    #publicationAttention = rank2attention(editorialRank, autoRank)\n",
    "    sentences = list(text2sentences(articleId, articleText))\n",
    "    if sentences:\n",
    "        yield Row(articleId=articleId, mediaType=mediaType, title=title, author=author,\n",
    "                  publication=publication, editorialRank=editorialRank, autoRank=autoRank, sentences=sentences)\n",
    "\n",
    "def dicts2articles(dicts):\n",
    "    '''Generates articles from an iterable of dicts.'''\n",
    "    return itertools.chain(*[dict2article(d) for d in dicts])\n",
    "\n",
    "\n",
    "# ### From Test Dataset\n",
    "# \n",
    "# One hour of moreover articles on:\n",
    "# * content market\\*\n",
    "# * native ad\\*\n",
    "# * sponsored content\n",
    "# \n",
    "# JQ command for selecting articles:\n",
    "# \n",
    "# ```\n",
    "# time cat moreover-2016-02-17T12/*/*/* | jq -c '.articles[] | select(has(\"content\") and .language == \"English\") | select(.content | test(\"content\\\\s+market|native\\\\s+ad|sponsored\\\\s+content\"; \"i\"))' > cm_articles.json\n",
    "# ```\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "def getOneTestArticle():\n",
    "    with open('cm_articles.json') as f:\n",
    "        for line in f:\n",
    "            return json.loads(line)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "def createTestArticles(outputPath):\n",
    "    timerStart()\n",
    "    articles = sqlContext.createDataFrame(sc.textFile('cm_articles.json').map(json.loads).flatMap(dict2article))\n",
    "    articles.write.parquet(outputPath)\n",
    "    timerEnd()\n",
    "    return articles\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "# articles = createTestArticles('test_articles.parquet').cache()\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "# articles = sqlContext.read.parquet('test_articles.parquet').cache()\n",
    "\n",
    "\n",
    "# ### From S3 Dataset\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def packet2articles(key):\n",
    "    print 'Processing: {}'.format(key)\n",
    "    s3obj = boto3.resource('s3').Object(bucket_name='***', key=key)\n",
    "    packet = json.loads(s3obj.get()['Body'].read().decode('utf-8'))\n",
    "    return dicts2articles(packet.get('articles', list()))\n",
    "\n",
    "def createS3Articles(startKey, endKey, outputPath):\n",
    "    timerStart()\n",
    "    keys = list(scanMoreoverKeys(startKey, endKey))\n",
    "    #print 'Key list: {}'.format(keys)\n",
    "    articles = sqlContext.createDataFrame(sc.parallelize(keys, 1000).flatMap(packet2articles))\n",
    "    articles.write.parquet(outputPath)\n",
    "    timerEnd()\n",
    "    return articles\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "# articles = createS3Articles('2016/02/17', '2016/02/18', 's3://dkddata/2_18PezzedArts.parquet').cache()\n",
    "#testArts = createS3Articles('2016/02/17/01/00/', '2016/02/17/01/01', 's3://dkddata/tmp_1month/test.parquet')\n",
    "#0:02:02.870538 with 20 nodes\n",
    "# In[9]:\n",
    "\n",
    "# testRead = sqlContext.read.option(\"mergeSchema\", \"false\").parquet('s3://dkddata/tmp_1month/test.parquet').cache()\n",
    "\n",
    "\n",
    "# ## Score Mentions\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "def mentionCount(article):\n",
    "    score = 0\n",
    "    for s in article['sentences']:\n",
    "        for m in s['mentions']:\n",
    "            score += m['preMentionScore']\n",
    "    return score\n",
    "\n",
    "def countMentionsByPub(articles):\n",
    "    '''Creates a table of mention counts for each publication in articles.'''\n",
    "    createTable('mentionCountsByPub', articles\n",
    "                .map(lambda a: (a.publication, mentionCount(a)))\n",
    "                .reduceByKey(lambda x,y: x + y),\n",
    "                ['publication', 'preMentionScoreSum'])\n",
    "\n",
    "def rank2attention(editorialRank=0, autoRank=0):\n",
    "    if editorialRank > 0:\n",
    "        return {\n",
    "            1:3000, 2:1600, 3:160, 4:130, 5:130\n",
    "        }.get(editorialRank, 130)\n",
    "    else:\n",
    "        return {\n",
    "            1:1400, 2:475, 3:200, 4:85, 5:50, 6:30, 7:25, 8:25, 9:25, 10:25\n",
    "        }.get(autoRank, 25)\n",
    "\n",
    "def scoreSentenceMentions(sentence, mentionScoreMultiplier):\n",
    "    '''Assigns mentionScore to each mention in sentence.'''\n",
    "    s = sentence.asDict()\n",
    "    s['mentions'] = [Row(theme=m['theme'], score=m['preMentionScore'] * mentionScoreMultiplier) for m in s['mentions']]\n",
    "    return Row(**s)\n",
    "\n",
    "def scoreArticleMentions(articleAndMentionScore):\n",
    "    '''\n",
    "    Assigns articleAndMentionScore.mentionScore as the score of each mention in article.\n",
    "    Returns article (without mentionScore column).\n",
    "    '''\n",
    "    a = articleAndMentionScore.asDict()\n",
    "    mentionScoreMultiplier = a.get('mentionScoreMultiplier', 0)\n",
    "    a['sentences'] = [scoreSentenceMentions(s, mentionScoreMultiplier) for s in a['sentences']]\n",
    "    del a['mentionScoreMultiplier']\n",
    "    return Row(**a)\n",
    "\n",
    "def scoreArticles(articles):\n",
    "    try:\n",
    "        sqlContext.dropTempTable('articles')\n",
    "    except:\n",
    "        pass\n",
    "    articles.registerTempTable('articles')\n",
    "    sqlContext.registerFunction('rank2attention', rank2attention, pyspark.sql.types.IntegerType())\n",
    "    countMentionsByPub(articles)\n",
    "    scoredArticles = createTable(\n",
    "        'scoredArticles',\n",
    "        sqlContext.sql(\n",
    "            'select a.*, 1.0 * rank2attention(a.editorialRank, a.autoRank) / p.preMentionScoreSum as mentionScoreMultiplier ' +\n",
    "            'from articles a, mentionCountsByPub p where a.publication=p.publication'\n",
    "        ).map(scoreArticleMentions))\n",
    "    return scoredArticles\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "def createScoredArticles(articles, outputPath):\n",
    "    timerStart()\n",
    "    scoredArticles = scoreArticles(articles)\n",
    "    scoredArticles.write.parquet(outputPath)\n",
    "    timerEnd()\n",
    "    return scoredArticles\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "# scoredArticles = createScoredArticles(articles, 'scoredArticles.parquet').cache()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# scoredArticles = sqlContext.read.parquet('scoredArticles.parquet').cache()\n",
    "\n",
    "#scoredArticles = sqlContext.read.option(\"mergeSchema\", \"false\").parquet('s3://dkddata/2_18ScoredArts.parquet').cache()\n",
    "# ## Indexing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "keyPairs = [('2016-03-{:02d}'.format(i),'2016-03-{:02d}'.format(i+1)) for i in range(1, 29)]\n",
    "for pair in keyPairs:\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        startKey = pair[0]\n",
    "        endKey = pair[1]\n",
    "        keys = list(scanOpeKeys(startKey, endKey))\n",
    "        pkeys = sc.parallelize(keys,1000)\n",
    "        parsed = pkeys.map(bundleArticleData)\n",
    "        #parsed.cache()\n",
    "        #parsed.count()\n",
    "        #failedKeys = sqlContext.createDataFrame(parsed.filter(lambda x: 'failedKey' in x.asDict()))\n",
    "        #failedKeys.write.parquet('s3://***/monthScoring/failedKeys/failedKeys_' + startKey)\n",
    "        arts = sqlContext.createDataFrame(parsed.filter(lambda x: 'failed' not in x.asDict()))\n",
    "        arts.cache()\n",
    "        arts.count()\n",
    "        #createScoredArticles(articles,'s3://***/monthScoring/scoredArts/scoredArts_' + startKey)\n",
    "        scoreArticles(arts).write.parquet('s3://***/monthScoring/scoredArts2/scoredArts_' + startKey)\n",
    "        #parsed.unpersist()\n",
    "        arts.unpersist()\n",
    "        print '{} is done in {} seconds'.format(startKey, time.time()-t0)\n",
    "    except:\n",
    "        print startKey\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Querying OPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "def queryArts(article, query):\n",
    "    meta = article.asDict()\n",
    "    del meta['sentences']\n",
    "    for sent in article['sentences']:\n",
    "        sentText = sent['sentenceText'].lower()\n",
    "        for term in query:\n",
    "            if term in sentText:\n",
    "                sentWithMeta = sent.asDict()\n",
    "                sentWithMeta.update(meta)\n",
    "                yield Row(**sentWithMeta)\n",
    "\n",
    "def sentencesToMention(sent):\n",
    "    meta = sent.asDict()\n",
    "    del meta['mentions']\n",
    "    for m in sent.asDict().get('mentions',[]):\n",
    "        mention = m['theme']\n",
    "        score = m['score']\n",
    "        temp = dict(meta)\n",
    "        temp.update({'mention':mention, 'score':score})\n",
    "        yield Row(**temp)\n",
    "\n",
    "\n",
    "artDir = ['s3://***/monthScoring/scoredArts2/scoredArts_2016-03-{:02d}'.format(i) for i in range(1, 29)]\n",
    "query = ['string 1', 'string 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t=time.time()\n",
    "articles = sqlContext.read.option(\"mergeSchema\", \"false\").parquet(*artDir)\n",
    "queriedMentions = articles.flatMap(lambda x: queryArts(x,query))\n",
    "mentionsDf = sqlContext.createDataFrame(queriedMentions.coalesce(100).flatMap(sentencesToMention))\n",
    "mentionsDf.write.parquet('s3://***/monthScoring/contentMarketingScoredMentions')\n",
    "time.time() - t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Twitter Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import json\n",
    "\n",
    "def parsePersonaDat(row):\n",
    "    parsed = row.split('\\t')\n",
    "    if len(parsed) != 3:\n",
    "        return Row(failed='incorrectLength',parsed=parsed)\n",
    "    return Row(profileId=parsed[0], handle=parsed[1], followerCount=parsed[2])\n",
    "\n",
    "data = sc.textFile('s3://***/twitter_follower_counts/*').map(parsePersonaDat)\n",
    "twits = sqlContext.createDataFrame(data.filter(lambda x: 'failed' not in x.asDict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fluff(flat):\n",
    "    deep = dict()\n",
    "    for key in flat:\n",
    "        d = deep\n",
    "        path = key.split('/')\n",
    "        while len(path) > 1:\n",
    "            p = path.pop(0)\n",
    "            d[p] = d.get(p, dict())\n",
    "            d = d[p]\n",
    "        d[path[0]] = flat[key]\n",
    "    return deep\n",
    "\n",
    "topicTweets = sqlContext.createDataFrame(sc.textFile('s3://***/monthScoring/cmTwitter.json').map(json.loads).map(fluff))\n",
    "topicTweets2 = topicTweets.withColumn('profileId',topicTweets.authorId[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicTweetsWithFollowers = topicTweets2.join(twits, twits.profileId == topicTweets2.profileId, 'left_outer').drop(twits.profileId)\n",
    "topicTweetsWithFollowers.write.parquet('s3://***/monthScoring/cmTwitterWithFollowers2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicTweetsWithFollowers = sqlContext.read.option(\"mergeSchema\", \"false\").parquet('inputs/cmTwitterWithFollowers2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "def extractTweetsAndScores(row, followerDivisor=18000):\n",
    "    payloadList = ast.literal_eval(row['themePayload'])\n",
    "    payLength = len(payloadList)\n",
    "    if payLength == 0:\n",
    "        return\n",
    "    score = float(row['followerCount'])/(followerDivisor * payLength)\n",
    "    date = row['timestamp'][:10]\n",
    "    sentenceText = row['sentenceText']\n",
    "    author = row['authorText'][0]\n",
    "    sentenceId = row['sentenceId']\n",
    "    for d in payloadList:\n",
    "        twitterType = d['SOURCE']\n",
    "        mention = d['TEXT']\n",
    "        yield(Row(mediaType=\"Twitter\", sentenceId=sentenceId, date=date, sentenceText=sentenceText, author=author, score=score, twitterType=twitterType, mention=mention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.createDataFrame(topicTweetsWithFollowers.flatMap(extractTweetsAndScores)).write.parquet('outputs/cmTwitterScored')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Adjust for daily flucations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pubPubs = sqlContext.read.option(\"mergeSchema\", \"false\").parquet('inputs/pubPublishingByDay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1403519"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubPubs.cache()\n",
    "pubPubs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def opeAddDates(row):\n",
    "    d = row.asDict()\n",
    "    d['date'] = d['articleId'][:10]\n",
    "    return Row(**d)\n",
    "\n",
    "opeRaw = sqlContext.createDataFrame(sqlContext.read.option(\"mergeSchema\", \"false\").parquet('inputs/contentMarketingScoredMentions').map(opeAddDates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMultiplier(row, numDays = 28):\n",
    "    numDays = float(numDays)\n",
    "    d = row.asDict()\n",
    "    if d['pubArtsInMonth'] < 100:\n",
    "        d['dayMultiplier'] = 1.0\n",
    "    else:\n",
    "        d['dayMultiplier'] = d['pubsArticlesInDay'] / (d['pubArtsInMonth'] / numDays)\n",
    "    d.pop('pubsArticlesInDay')\n",
    "    d.pop('pubArtsInMonth')\n",
    "    d.pop('autoRank')\n",
    "    d.pop('editorialRank')\n",
    "    d.pop('attention')\n",
    "    return(Row(**d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totPubCounts = pubPubs.groupBy(['publication', 'mediaType']).sum('pubsArticlesInDay').withColumnRenamed('sum(pubsArticlesInDay)','pubArtsInMonth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pubsCounts = sqlContext.createDataFrame(pubPubs.join(totPubCounts, ['publication', 'mediaType'], 'left_outer').map(getMultiplier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, publication: string, mediaType: string, articleId: string, author: string, autoRank: bigint, editorialRank: bigint, mention: string, score: double, sentenceId: string, sentenceText: string, title: string, dayMultiplier: double]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergedOpePubCount = opeRaw.join(pubsCounts, ['date', 'publication', 'mediaType'], 'left_outer')\n",
    "mergedOpePubCount.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjustScores(row):\n",
    "    d = row.asDict()\n",
    "    multiplier = d.pop('dayMultiplier')\n",
    "    if not multiplier:\n",
    "        multiplier = 1.0\n",
    "    d['score'] = d.pop('score') * multiplier\n",
    "    return Row(**d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adjustedMentions = sqlContext.createDataFrame(mergedOpePubCount.map(adjustScores))\n",
    "adjustedMentions.write.parquet('outputs/contentMarketingScoredMentions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Merge OPE & Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepOpe(row):\n",
    "    d = row.asDict()\n",
    "    d['date'] = d['articleId'][:10]\n",
    "    d['mentionType'] = 'NA'\n",
    "    d['source'] = d.pop('mediaType')\n",
    "    d.pop('editorialRank')\n",
    "    d.pop('autoRank')\n",
    "    return Row(**d)\n",
    "\n",
    "def prepTwit(row):\n",
    "    d = row.asDict()\n",
    "    d['mentionType'] = d.pop('twitterType')\n",
    "    d['source'] = d.pop('mediaType')\n",
    "    d['articleId'] = 'Tweet, s=' + d['sentenceId']\n",
    "    d['title'] = 'Tweet, s=' + d['sentenceId']\n",
    "    d['publication'] = d['author']\n",
    "    return Row(**d)\n",
    "\n",
    "#articleId: string, author: string, autoRank: bigint, date: string, editorialRank: bigint, mediaType: string, \n",
    "##                        mention: string, publication: string, score: double, sentenceId: string, sentenceText: string, title: string\n",
    "#                                                mentionType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitRaw = sqlContext.createDataFrame(sqlContext.read.option(\"mergeSchema\", \"false\").parquet('outputs/cmTwitterScored').map(prepTwit))\n",
    "opeRaw = sqlContext.createDataFrame(sqlContext.read.option(\"mergeSchema\", \"false\").parquet('outputs/contentMarketingScoredMentions').map(prepOpe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mergedScores = twitRaw.unionAll(opeRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mergedScores.toPandas().to_excel('cmScoresAdjusted.xlsx', sheet_name = 'Sheet1', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80217"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergedScores.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Look at Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------+-------------+---------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|           articleId| author|autoRank|editorialRank|mediaType|             mention|publication|               score|          sentenceId|        sentenceText|               title|\n",
      "+--------------------+-------+--------+-------------+---------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|2016-03-01-14-13-...|Unknown|     -10|            3|     news|      store analytic|   BioSpace|0.001055191598905239|2016-03-01-14-13-...|With our comprehe...|69% of Shoppers W...|\n",
      "|2016-03-01-14-13-...|Unknown|     -10|            3|     news|    store investment|   BioSpace|0.001055191598905239|2016-03-01-14-13-...|With our comprehe...|69% of Shoppers W...|\n",
      "|2016-03-01-14-13-...|Unknown|     -10|            3|     news|      content market|   BioSpace|0.001055191598905239|2016-03-01-14-13-...|With our comprehe...|69% of Shoppers W...|\n",
      "|2016-03-01-14-13-...|Unknown|     -10|            3|     news|customer experien...|   BioSpace|0.001055191598905239|2016-03-01-14-13-...|With our comprehe...|69% of Shoppers W...|\n",
      "|2016-03-01-14-13-...|Unknown|     -10|            3|     news|complex retail en...|   BioSpace|0.001055191598905239|2016-03-01-14-13-...|With our comprehe...|69% of Shoppers W...|\n",
      "+--------------------+-------+--------+-------------+---------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = sqlContext.read.option(\"mergeSchema\", \"false\").parquet('inputs/contentMarketingScoredMentions')\n",
    "inputs.cache()\n",
    "inputs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62672"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs.toPandas().to_excel('cmScoredMentions' + '.xls', sheet_name = 'Sheet1', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "def addDate(mention):\n",
    "    mentDict = mention.asDict()\n",
    "    mentDict['date'] = mentDict.get('sentenceId')[:10]\n",
    "    return(Row(**mentDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mentions = sqlContext.createDataFrame(inputs.map(addDate))\n",
    "mentions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------+----------+-------------+---------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|           articleId| author|autoRank|      date|editorialRank|mediaType|             mention|publication|               score|          sentenceId|        sentenceText|               title|\n",
      "+--------------------+-------+--------+----------+-------------+---------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|2016-03-01-14-13-...|Unknown|     -10|2016-03-01|            3|     news|      store analytic|   BioSpace|0.001055191598905239|2016-03-01-14-13-...|With our comprehe...|69% of Shoppers W...|\n",
      "|2016-03-01-14-13-...|Unknown|     -10|2016-03-01|            3|     news|    store investment|   BioSpace|0.001055191598905239|2016-03-01-14-13-...|With our comprehe...|69% of Shoppers W...|\n",
      "|2016-03-01-14-13-...|Unknown|     -10|2016-03-01|            3|     news|      content market|   BioSpace|0.001055191598905239|2016-03-01-14-13-...|With our comprehe...|69% of Shoppers W...|\n",
      "|2016-03-01-14-13-...|Unknown|     -10|2016-03-01|            3|     news|customer experien...|   BioSpace|0.001055191598905239|2016-03-01-14-13-...|With our comprehe...|69% of Shoppers W...|\n",
      "|2016-03-01-14-13-...|Unknown|     -10|2016-03-01|            3|     news|complex retail en...|   BioSpace|0.001055191598905239|2016-03-01-14-13-...|With our comprehe...|69% of Shoppers W...|\n",
      "+--------------------+-------+--------+----------+-------------+---------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mentions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      date|         dateScore|\n",
      "+----------+------------------+\n",
      "|2016-03-01| 665.6508999256231|\n",
      "|2016-03-02|  846.961384535139|\n",
      "|2016-03-03| 909.0376272354772|\n",
      "|2016-03-04| 593.9498856296218|\n",
      "|2016-03-05| 91.41313889301027|\n",
      "|2016-03-06| 111.7143222887835|\n",
      "|2016-03-07| 900.5253512237755|\n",
      "|2016-03-08| 799.8537637547616|\n",
      "|2016-03-09|1206.3520678409448|\n",
      "|2016-03-10|  676.203058148176|\n",
      "|2016-03-11| 733.0035486823363|\n",
      "|2016-03-12| 90.24831020583008|\n",
      "|2016-03-13|200.28356574169186|\n",
      "|2016-03-14| 504.4277031801616|\n",
      "|2016-03-15|1385.7246504885918|\n",
      "|2016-03-16|1227.9653965832435|\n",
      "|2016-03-17|1341.9984840220375|\n",
      "|2016-03-18| 714.4126330280028|\n",
      "|2016-03-19|200.82049866570136|\n",
      "|2016-03-20| 280.6937741884606|\n",
      "|2016-03-21| 606.6077094157159|\n",
      "|2016-03-22|1375.8682602446008|\n",
      "|2016-03-23|2430.0988696698714|\n",
      "|2016-03-24| 749.7466996086685|\n",
      "|2016-03-25|1258.1656137078212|\n",
      "|2016-03-26| 517.5927715153019|\n",
      "|2016-03-27|  81.8523662813033|\n",
      "|2016-03-28| 865.1212742786892|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mentions.groupBy('date').sum('score').withColumnRenamed('sum(score)','dateScore').show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, rank\n",
    "ranks = mentions.groupBy('mention').sum('score').withColumnRenamed('sum(score)','overalScore').withColumn('overalRank',rank().over(Window.partitionBy().orderBy(desc('overalScore')))).filter('overalRank <= 15')\n",
    "dateMentions = mentions.groupBy(['date','mention']).sum('score').withColumnRenamed('sum(score)','score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----------+----------+-------------------+-------+\n",
      "|             mention|       overalScore|overalRank|      date|              score|dayRank|\n",
      "+--------------------+------------------+----------+----------+-------------------+-------+\n",
      "|      content market|3457.4538754036957|         1|2016-03-01|  82.68577399059035|      1|\n",
      "|           advertise| 287.8561747449625|         3|2016-03-01| 18.690894383073065|      2|\n",
      "|             content|161.12239840935393|         8|2016-03-01|  9.223506605259598|      3|\n",
      "|            marketer|126.85160943878743|        12|2016-03-01|  8.050745761075522|      4|\n",
      "|content market ef...|125.92245256747059|        13|2016-03-01|  7.725669440442079|      5|\n",
      "|Content Marketing...|120.89069726715745|        14|2016-03-01| 7.1719926980333355|      6|\n",
      "|    native advertise| 485.1678118247768|         2|2016-03-01|  6.665304878956423|      7|\n",
      "|content market st...|  251.370269509646|         4|2016-03-01|  6.452226100676326|      8|\n",
      "|                2016|165.80005163200693|         7|2016-03-01|  4.944258702992836|      9|\n",
      "|   Content Marketing| 181.5995932726816|         6|2016-03-01|  4.325749584525692|     10|\n",
      "|     market strategy|132.19742818701457|        11|2016-03-01| 0.8351375426232884|     11|\n",
      "|               brand|151.83233425437183|         9|2016-03-01| 0.3705704234810732|     12|\n",
      "|         market play|116.20960051452096|        15|2016-03-02|  114.7492871413194|      1|\n",
      "|      content market|3457.4538754036957|         1|2016-03-02|  86.16594068389523|      2|\n",
      "|   Content Marketing| 181.5995932726816|         6|2016-03-02| 11.242394369873136|      3|\n",
      "|     market strategy|132.19742818701457|        11|2016-03-02| 10.344499326591304|      4|\n",
      "|Content Marketing...|120.89069726715745|        14|2016-03-02|  9.958628327368153|      5|\n",
      "|    native advertise| 485.1678118247768|         2|2016-03-02|  9.410451153952865|      6|\n",
      "|            marketer|126.85160943878743|        12|2016-03-02|  8.317393741309433|      7|\n",
      "|           advertise| 287.8561747449625|         3|2016-03-02|  5.162794629067301|      8|\n",
      "|content market st...|  251.370269509646|         4|2016-03-02| 3.7246611624153894|      9|\n",
      "|             content|161.12239840935393|         8|2016-03-02| 3.6566624376198074|     10|\n",
      "|                2016|165.80005163200693|         7|2016-03-02| 1.6716044390769074|     11|\n",
      "|content market ef...|125.92245256747059|        13|2016-03-02|  0.822042770940793|     12|\n",
      "|      content market|3457.4538754036957|         1|2016-03-03| 126.70279431543247|      1|\n",
      "|    native advertise| 485.1678118247768|         2|2016-03-03| 12.316081831923453|      2|\n",
      "|            marketer|126.85160943878743|        12|2016-03-03| 11.430321041705515|      3|\n",
      "|content market ef...|125.92245256747059|        13|2016-03-03|  8.169694268178155|      4|\n",
      "|                2016|165.80005163200693|         7|2016-03-03|  8.007457224481161|      5|\n",
      "|Content Marketing...|120.89069726715745|        14|2016-03-03|   4.70404751675623|      6|\n",
      "|             content|161.12239840935393|         8|2016-03-03|   3.94562157718048|      7|\n",
      "|     market strategy|132.19742818701457|        11|2016-03-03| 3.3934734401701916|      8|\n",
      "|               brand|151.83233425437183|         9|2016-03-03|  2.662299935818129|      9|\n",
      "|   Content Marketing| 181.5995932726816|         6|2016-03-03|  2.074454950137198|     10|\n",
      "|content market st...|  251.370269509646|         4|2016-03-03| 1.9876675232871568|     11|\n",
      "|           advertise| 287.8561747449625|         3|2016-03-03|0.30754753552178743|     12|\n",
      "|      content market|3457.4538754036957|         1|2016-03-04|  67.17908082658181|      1|\n",
      "|           advertise| 287.8561747449625|         3|2016-03-04|  43.80183708229254|      2|\n",
      "|    native advertise| 485.1678118247768|         2|2016-03-04|   13.6047576279573|      3|\n",
      "|Content Marketing...|120.89069726715745|        14|2016-03-04| 13.266365070127359|      4|\n",
      "|   Content Marketing| 181.5995932726816|         6|2016-03-04|  7.614868942726222|      5|\n",
      "|content market st...|  251.370269509646|         4|2016-03-04|  5.312329335500438|      6|\n",
      "|             content|161.12239840935393|         8|2016-03-04|  4.514724582118815|      7|\n",
      "|                2016|165.80005163200693|         7|2016-03-04|  2.125240822914512|      8|\n",
      "|     market strategy|132.19742818701457|        11|2016-03-04| 0.9616206762761894|      9|\n",
      "|               brand|151.83233425437183|         9|2016-03-04|  0.857828524636671|     10|\n",
      "|            marketer|126.85160943878743|        12|2016-03-04| 0.6154930464438043|     11|\n",
      "|                 FTC| 139.6252872423636|        10|2016-03-04|0.41811708758754634|     12|\n",
      "|content market ef...|125.92245256747059|        13|2016-03-04| 0.0565518755795444|     13|\n",
      "|      content market|3457.4538754036957|         1|2016-03-05|   9.07292332040964|      1|\n",
      "|Content Marketing...|120.89069726715745|        14|2016-03-05|  2.261667546524016|      2|\n",
      "|content market ef...|125.92245256747059|        13|2016-03-05| 2.0048126596278775|      3|\n",
      "|content market st...|  251.370269509646|         4|2016-03-05| 0.9609860416037075|      4|\n",
      "|   Content Marketing| 181.5995932726816|         6|2016-03-05| 0.9512073862266682|      5|\n",
      "|                2016|165.80005163200693|         7|2016-03-05| 0.8465698023435448|      6|\n",
      "|    native advertise| 485.1678118247768|         2|2016-03-05|0.24124798570455444|      7|\n",
      "|             content|161.12239840935393|         8|2016-03-05|0.18580486004728233|      8|\n",
      "|    native advertise| 485.1678118247768|         2|2016-03-06| 16.339361428099455|      1|\n",
      "|      content market|3457.4538754036957|         1|2016-03-06|  9.131344667806063|      2|\n",
      "|Content Marketing...|120.89069726715745|        14|2016-03-06| 0.8103758046732588|      3|\n",
      "|            marketer|126.85160943878743|        12|2016-03-06|0.37888663653905885|      4|\n",
      "|   Content Marketing| 181.5995932726816|         6|2016-03-06| 0.2939348428565086|      5|\n",
      "|content market ef...|125.92245256747059|        13|2016-03-06|0.11975935141905508|      6|\n",
      "|             content|161.12239840935393|         8|2016-03-06|0.07450461398604304|      7|\n",
      "|     market strategy|132.19742818701457|        11|2016-03-06| 0.0422912103803649|      8|\n",
      "|      content market|3457.4538754036957|         1|2016-03-07| 123.95549637241297|      1|\n",
      "|             content|161.12239840935393|         8|2016-03-07|  18.27309110518786|      2|\n",
      "|                2016|165.80005163200693|         7|2016-03-07| 18.218883872957193|      3|\n",
      "|    native advertise| 485.1678118247768|         2|2016-03-07| 10.994460851286773|      4|\n",
      "|   Content Marketing| 181.5995932726816|         6|2016-03-07| 10.325033959394059|      5|\n",
      "|content market ef...|125.92245256747059|        13|2016-03-07|  9.799285856506156|      6|\n",
      "|            marketer|126.85160943878743|        12|2016-03-07|  7.093478758894843|      7|\n",
      "|content market st...|  251.370269509646|         4|2016-03-07| 5.1957179002749605|      8|\n",
      "|Content Marketing...|120.89069726715745|        14|2016-03-07|  5.062023355106238|      9|\n",
      "|                 FTC| 139.6252872423636|        10|2016-03-07| 0.9872779291469923|     10|\n",
      "+--------------------+------------------+----------+----------+-------------------+-------+\n",
      "only showing top 75 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dayRanks = ranks.join(dateMentions, 'mention', 'left_outer').withColumn('dayRank',rank().over(Window.partitionBy('date').orderBy(desc('score'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ScratchCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
